\documentclass[twocolumn]{article}

%%% PACKAGES %%%
\usepackage[a4paper, margin=0.75in]{geometry}
\usepackage{setspace}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage{float}
\usepackage{stfloats}
\usepackage{blindtext}
\usepackage{caption}
\usepackage{titlesec}
\usepackage{svg}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{arrows, arrows.meta, calc, positioning, quotes, shapes}
\usepackage{amsmath}
\usepackage{amssymb}

%%% CONFIG %%%
\renewcommand{\thesection}{\Alph{section}}
\renewcommand{\thesubsection}{\arabic{subsection}}
\titleformat{\section}{\centering\large\bfseries}{Part \thesection}{0em}{}{}
\titleformat{\subsection}{\bfseries}{\thesubsection.}{0.5em}{}{}
\setlist[enumerate]{
    label=(\alph*),
    wide=0pt,
}
\setlength{\abovecaptionskip}{5pt}
\setlength{\belowcaptionskip}{0pt}

%%% TITLE METADATA %%%
\title{CSC311 Final Report: Project Option 1}
\author[]{Leo Peckham}
\author[]{Longyue Wang}
\author[]{Gursewak Sandhu}
\affil[]{}
\date{}

\begin{document}
\maketitle

\section{}

\subsection{$k$-nearest neighbours}

\begin{enumerate}

\item
~
\begin{figure}[!h]
    \centering
    \includesvg[width=\linewidth]{A1a}
    {\footnotesize
        \setlength{\tabcolsep}{3pt}
        \begin{tabular}{ |c|c|c|c|c|c|c| } 
            \hline
            \textbf{$k$-value} & 1 & 6 & {\bf 11} & 16 & 21 & 26 \\ 
            \textbf{Accuracy} & 0.628 & 0.677 & {\bf 0.689}
                              & 0.675 & 0.668 & 0.651 \\ 
            \hline
        \end{tabular}
    }
    \captionof{figure}{Accuracies of user-clustering {\sc knn}}
    \label{fig:userclustering}
\end{figure}


\item
From this data, we can see that the $k^*$ that gave us the best validation
accuracy was $k^* = 11$. Running on test we achieve an accuracy of $0.683$.

\item
~
\begin{figure}[H]
    \centering
    \includesvg[width=\linewidth]{A1c}
    {\footnotesize
        \setlength{\tabcolsep}{3pt}
        \begin{tabular}{ |c|c|c|c|c|c|c| } 
            \hline
            \textbf{$k$-value} & 1 & 6 & 11 & 16 & {\bf 21} & 26 \\ 
            \textbf{Accuracy} & 0.618 & 0.660 & 0.680
                              & 0.687 & {\bf 0.690} & 0.689 \\ 
            \hline
        \end{tabular}
    }
    \captionof{figure}{Accuracies of question-clustering {\sc knn}}
    \label{fig:questionclustering}
\end{figure}

Question-based clustering assumes that if two questions have very similar
distributions of answers across the known students, then the questions will
behave similarly for new students. Intuitively, if two questions ask about a
specific theorem, then the same students who get the first one wrong because
they forgot the statement of the theorem will get the second wrong as well.

From the data in Figure \ref{fig:questionclustering}, we can see that the $k^*$ that gave us the best validation
accuracy was $k^* = 21$. Running on test we achieve an accuracy of $0.670$.

\item
The performances across the board are similar. The user-based approach does
better on test, but only by around a tenth of a percent accuracy. The graphs
for different $k$-values do look quite different, though, with the
user-clustering in Figure \ref{fig:userclustering} falling off much faster for
high $k$-values than the question-clustering in figure
\ref{fig:questionclustering} .

\item

This method relies on a sufficient amount of data to be able to accurately
cluster. If there is an example with a significant amount held-out, it will
become inaccurate because many nearby users/questions will look similar.

The dimension of the features is also a problem. First, it can be extremely
high, being either the number of students or the number of questions. This
poses a problem for a {\sc knn} approach, especially one that uses a Euclidean
distance like this one. Mapping to a smaller latent space first and using
another distance function would help alieviate this problem. Second, if we add
new students or questions to the dataset, the very {\it dimensionality} of our
data will change. This makes it more difficult to improve our model with new
data; mapping to a laten space would also fix this issue.

\end{enumerate}

\subsection{Item response theory}
\item 
(a) 
\[
p(c_{ij} = 1 \mid \theta_i, \beta_j)
= \sigma(\theta_i - \beta_j)
= \frac{\exp(\theta_i - \beta_j)}{1 + \exp(\theta_i - \beta_j)}
\]

\[
\log p(C \mid \theta, \beta)
= \sum_{i,j} \left[ c_{ij} \log \sigma(\theta_i - \beta_j)
+ (1 - c_{ij}) \log (1 - \sigma(\theta_i - \beta_j)) \right]
\]

Derivative w.r.t. \(\theta_i\):
\[
\frac{\partial \log p(C \mid \theta, \beta)}{\partial \theta_i}
= \sum_j \left[ c_{ij} - \sigma(\theta_i - \beta_j) \right]
\]

Derivative w.r.t. \(\beta_j\):
\[
\frac{\partial \log p(C \mid \theta, \beta)}{\partial \beta_j}
= \sum_i \left[ \sigma(\theta_i - \beta_j) - c_{ij} \right]
\]


\item 
(b) After some experimenting we found that the best results occured with our Learning rate
set to 0.01 and the number of iterations set to 200. Two hundred was found to be sufficient
as both the curves can be seen plateauing, and our 1000 iterations test found no significant changes.
\includesvg{A2b-2}

\item 
(c) After running the IRT model with a learning rate of 0.01 and 200 iterations we got the following.
Our IRT model scored a validation accuracy of 0.707, and a test accuracy of 0.708

\item 
(d) We selected 3 questions and put them against our trained model. 
\includesvg{A2d}
From our graph, the x-axis represents $\theta$, which is the ability of the student, and the y-axis represents the predicted probability that a student answers the questions correctly. The plot makes use of scatter plot points to mark each individual students.

Each curve has a S shape of the logistic function that is used in our IRT model. The S shape occurs because students with abilities(theta) much lower than the difficulty of the question have probabilites near 0 while, students with greater ability have probabilities closer to 1. And the transition phase of these points can be seen when the students ability matches with the difficulty of our questions. 

In the 3 curves we have plotted, you may notice that curves may be to the left or right of each other. The curves positioning is determined by the difficulty of the question. In our graph the orange curve occurs to the right of the blue and green curves, which means the orange question would require a greater student ability to get correct compared to the blue and green curves. 


\subsection{Matrix factorization}

\begin{enumerate}

\item 
~
\vspace{-0.2cm}
\begin{mcfigure}
    \centering
    {\footnotesize
        \setlength{\tabcolsep}{3pt}
        \begin{tabular}{ |c|c|c|c|c|c|c| } 
            \hline
            \textbf{$k$-value} & 1 & {\bf6} &  11 & 16 & 21 & 26 \\ 
            \textbf{Val. Accuracy} & 0.642 & {\bf0.657} &  0.656
                              & 0.655 & 0.656 & 0.657 \\ 
            \textbf{Test Accuracy} & 0.647 & {\bf0.662} &  0.662
                              & 0.662 & 0.662 & 0.662 \\                  
            \hline
        \end{tabular}
    }
    \captionof{figure}{Accuracies of SVD}
    \label{fig:userclustering}
\end{mcfigure}

\item 

Currently, SVD is filling in the missing entries of the matrix with the average of the present values. This flattens the data and reduces the variance, with the flattening getting worse the sparser the matrix is before being filled in. This can lead to overfitting and reduced accuracy.

\item 

See code.

\item 

See code.

\item 

The chosen hyperparameters are 0.01 for the learning rate, and 4500 iterations. The final chosen validation accuracy is 0.592, and the loss evolved as follows: 
\begin{figure}[H]
    \centering
    \includesvg[width=\linewidth]{a3ie}
    \captionof{figure}{Training and Validation loss of ALS}
    \label{fig:questionclustering}
\end{figure}

\end{enumerate}

\subsection{Ensemble}
\item
We implemented the bagging ensemble using the following models: 2 IRT models with varying hyper parameters and a Matrix Factorization model. Each model was trained on a bootstrap of the training data, with 3 samples being generated with replacement. To determine our prediction each model produced a binary prediction of 0 or 1 to determine if the student got the question correct.  These sets of predictions were then summed up to enable a vote of whether the student got the question correctly. If 2 models said the student did then the ensemble model predicted the student got the question correct. 

From running our tests the following accuracies were obtained: Validation was 0.699 and Test was 0.695. This was actually slight lower than the best version of IRT we found which had Validation as 0.707 and Test as 0.708.
Which means it slightly underperformed in comparison. 

The reasons for the lower performance could be the selection of models. Both the IRTâ€™s were very similar even-though they had differing hyperparameters, which could lead to them having similar points of error. Additionally the method used top determine the vote may have been flawed with an average of the predicted probabilities possibly resulting in better results than the voted probabilty. Finally, the addition of the Matrix Factorization although beneficial for allowing for the recognition of more student patterns and having more diversity in the models, had a lower accuracy than the base IRT model which could have pulled down the accuracy of the ensemble. 



\section{}

\subsection{Motivation}

One of the major problems with the {\sc knn} model is that the number of
dimensions are very high. We have 1774 questions and 542 students, so based on
the way we do it we work in either a 1774 dimensional space, or a 542
dimensional space. Additionally, none of the approaches we take are robust to
the possibility of adding both more questions and more students, because this
increases the dimension in a way that would require retraining.

Ideally we would be able to work in some lower-dimensional latent space that
allows us to represent the qualities of the questions and the abilities of the
students, and then compute our hypotheses from that. This is similar to what
IRT does, where $\theta$ is our student ability, and $\beta$ is our question
difficulty. We essentially want to turn $\theta$ and $\beta$ into vectors in
this latent space.

\subsection{Design}

\begin{figure*}[ht]
    \centering
    \begin{tikzpicture}[
        auto,
        node distance = 11mm and 22mm,
        base/.style = {draw, rounded corners=5pt,
                       minimum height=1.5em, align=center},
        block/.style = {base, minimum width=10em},
        ]
        \coordinate (in);
        \node [block,right=of in] (A) {Binary Question\\Metadata ($\mathbf{Q}_1$)};
        \node [block,right=of A ] (B) {Latent Question\\Space ($\mathbf{L}$)};
        \node [block,below=2mm of A ] (C) {Sparse Train\\Matrix ($\mathbf{H}$)};
        \node [block,below=13mm of B ] (D) {Latent Student\\Space ($\mathbf{S}$)};
        \node [block,right=of D] (E) {IRT Parameters\\
            ($\mathbf{\Theta},\mathbf{B}$)};
        \node [block,above=13mm of E] (F) {Hypothesis};

        \draw [-{Latex[length=3mm]}, rounded corners] 
            (A) edge ["SVD Rank", "Reduction"']            (B)
            (B) edge ["Pooling"]                  (D)
            (E) edge ["IRT", "Decoding"'] (F)
            (C.east) -|                 (D)
            (B.east) -|  +(11mm,-34mm) -| (E)
            (D.east) -|  +(11mm,-11mm) -| (E)
            (C.south) |- +(0,-17mm) -|
                node[pos=0.375, auto=right]{Gradient Descent}  (E);
    \end{tikzpicture}
    \caption{General architecture of the proposed learning algorithm.}
    \label{fig:architecture}
\end{figure*}

Motivated by our desire to create a latent space, we lay out our architecture
in Figure \ref{fig:architecture}.

We start by constructing the latent space (which we will say has dimension $k$
from now on) with the question metadata. Each question has 388 subjects it can
be a part of, and so has 388 features. We take our question metadata $Q$ and we
encode the features into a binary representation, giving us a binary question
metadata matrix $\mathbf{Q}_1$ of shape $|q| \times |s| = 1774 \times 388$,
where $|q|$ is the number of questions and $|s|$ is the number of subjects. To
construct our latent question matrix $\mathbf{L}$ ($|q| \times k$), we do SVD
rank reduction $\mathbf{Q}_1$. That is:

\begin{align}
    \mathbf{Q}_1 &= \mathbf{U}\mathbf{\Sigma}\mathbf{V}^*\\
    \mathbf{L} &= \mathbf{U}_k\mathbf{\Sigma}_k
\end{align}

Where $\mathbf{U}$ and $\mathbf{V}$ are the left- and right-singular matrices
respectively, and $\mathbf{\Sigma}$ is the eigenvalue matrix. The $k$ subscript
means the matrix has been truncated. This formulation falls out from the shapes
of the matrices, but it also means that $\mathbf{L}$ is exactly the PCA score
matrix, which we are interpreting as a dimensionality reduction from $|s|$ to
$k$. over the rows of $\mathbf{Q}_1$

Next we construct the latent student/user matrix $\mathbf{S}$ of shape $|u|
\times k$ where $|u|$ is the number of users/students. We use the intuition
that the $k$ features of rows of $\mathbf{L}$ represent information about the
skillset required to answer the question corresponding to the given row. If a
student answers every question with a certain feature correct, then we will
want their magnitude in that feature to be very high. If a student gets half
right and half wrong, we will want it to cancel out and for their magnitude in
that feature to be very low. A naÃ¯ve way of getting these traits is to, for each student, pool over the latent question matrix $\mathbf{L}$ based on how they answered the question. To be precise, we use mean-pooling in the form:

\begin{align}
    \mathbf{S}_i = \frac{\sum_{j}^{} \mathbb{I}[\mathbf{H}_{ij} \ne \text{NaN}]
        (-1)^{\mathbf{H}_{ij} + 1}\mathbf{L}_{ij}}
        {\sum_{j}^{} \mathbb{I}[\mathbf{H}_{ij} \ne \text{NaN}]}
\end{align}

Where $\mathbb{I}$ is the indicator function.

$\mathbb{S}$ and $\mathbb{L}$ now hold the information we want to put into IRT,
but do not relate to each other in a way that would let us put it right into
our IRT evalutation function. Therefore, we first want to learn a mapping to
IRT compatible matrices $\mathbf{\Theta}$ and $\mathbf{B}$ respectively. We set
up the simplest such mapping:

\begin{align}
    \mathbf{\Theta} &= \mathbf{S} \mathbf{W}_\theta \\
    \mathbf{B} &= \mathbf{S} \mathbf{W}_\beta
\end{align}

Were $\mathbf{W}_\theta$ and $\mathbf{W}_\beta$ are matrices we will learn
using gradient descent. One would probably get much better results if using a
neural-network, or some other more sophisticated form of mapping, but we stick
to simplicity here.

For our gradient descent, we minimize negative log-likelihood on the cosine
similarity between $k$ dimensional vectors, similar to the treatment of IRT.

\begin{gather}
    \theta_i = \mathbf{S}_i \mathbf{W}_\theta,
    \beta_j = \mathbf{S}_i \mathbf{W}_\beta \\
    c_{ij} = \frac{\theta_i \beta_j}
        {\lVert\theta_i\rVert_2 \lVert\beta_j\rVert_2} \\
    g_{ij} = \sigma(ac_{ij} + b) \\
    \ell = - \sum_{i,j} \left[\mathbf{H}_{ij} \log g_{ij}
        (1 - \mathbf{H}_{ij}) \log (1 - g_{ij})\right] \\
    \frac{\partial\ell}{\partial\mathbf{W}_\theta} = 
        \frac{\partial\ell}{\partial\mathbf{\Theta}}
        \frac{\partial\mathbf{\Theta}}{\partial\mathbf{W}_\theta} =
        \mathbf{S}^\top
        \frac{\partial\mathbf{\Theta}}{\partial\mathbf{W}_\theta} \\
    \frac{\partial\mathbf{\Theta}}{\partial\mathbf{W}_\theta} =
        \left[\frac{\partial\theta_i}{\partial\mathbf{W}_\theta}\right]_i \\
    \frac{\partial\theta_i}{\partial\mathbf{W}_\theta} =
    \sum_j \left(g_{ij} - H_{ij}\right)
        \left(\frac{\beta_i}{\lVert\theta_i\rVert_2 \lVert\beta_j\rVert_2}
         - c_{ij}\frac{\theta_i}{\lVert{\theta_i}\rVert_2^2}\right)
\end{gather}


Where $\lVert\cdot\rVert_2$ is the $L^2$ norm, and $\sigma$ is the sigmoid
activation function. For the last line, we use the well known derivatives of
the negative log-likelihood and the cosine similarity, and a similar result
holds exchanging $\theta_i$ for $\beta_j$. $a$ and $b$ are parameters the model
trains along with $\mathbf{W}_\theta$ and $\mathbf{W}_\beta$ to prevent the
sigmoid from getting clustered too heavily around 0.5, since the cosine
similarity outputs small values.

We run gradient descent for 10,000 steps, using a learning rate of $1e^{-3}$
for everything except $a$ for which we use a learning rate of $1e^{-4}$. We
initialize the $\mathbf{W}$ matrices using a normal distribution centered
around 0 with a variance of 2 (we tuned this variance as a hyperparameter). We
start $a$ at 5 and $b$ at 0.

\begin{figure}[!ht]
    \centering
    \includesvg[width=\linewidth]{partb_k_search_by_40}
    \captionof{figure}{$k^*$ search for SVD rank reduction based on final
        accuracies of valid}
    \label{fig:kstar}
\end{figure}

For the dimension $k$ of our latent space, we did a hyperparameter search and
found that $k^* = 320$ was optimal. In Figure \ref{fig:kstar} we see the graph
of the search with increments of 40. Not shown is another search which we
repeated at increments of 10 zoomed in around 320 which also showed 320 to be
optimal.

\begin{figure}[!ht]
    \centering
    \includesvg[width=\linewidth]{partb_training_pre_dropout}
    \captionof{figure}{$k^*$ search for SVD rank reduction based on final
        accuracies of valid}
    \label{fig:predropout}
\end{figure}

Figure \ref{fig:predropout} shows the first round of training we did. The blue
line shows us the value of $\ell$ on the train set, and the red line shows us
the "score" (which is the same as for IRT) on the valid set. This got
accuracies of 79.9\% for train, and 64.8\% for both valid and test. This is
less than we hoped for, and as can be seen in the figure, shows clear signs of
overfitting.

\begin{figure}[!ht]
    \centering
    \includesvg[width=\linewidth]{partb_training_with_dropout}
    \captionof{figure}{$k^*$ search for SVD rank reduction based on final
        accuracies of valid}
    \label{fig:withdropout}
\end{figure}

To fix this, we added dropout on $\mathbf{B}$ and $\mathbf{B}$ at a 30\%
chance, which leads to the training curve in \ref{fig:withdropout}. This fixed
the problem, and by doing early stopping based on the valid score we got a
final train accuracy of 72.0\%, valid accuracy of 66.9\%, and test accuracy of
66.6\%.

\subsection{Comparison}

\item

\begin{mcfigure}
    \begin{tabular}{lllll}
                            & Original IRT & Modified IRT \\
        Validation Accuracy & 0.707 & 0.669\\
        Test Accuracy & 0.708 & 0.666\\
        & &  &  
    \end{tabular}
\end{mcfigure}

After comparing the base IRT model to our Extended version we found that the extend verision slightly underperfomed in comparison.

This could be due to many factors such as the complexity of this IRT mdoel in comparison to the base one, as we tried to take into account student ability more independentally across the data rather than having the student ability be set. 

\subsection{Limitations}

One situation in which we would expect our modification to perform poorly is in situations where the data is particularly sparse, that is to say, where most of the data is missing. 

Whenever we measure the magnitude in a feature, the empty spots in the matrix might skew what we expect to see from the student (i.e. they only have one question in that subject that they got wrong, whereas if all the data was there itâ€™d be mostly correct).

One way that we could potentially address this is an extension to extrapolate from more complete features, questions whose subjects are relatively similar. This way, we can be relatively confident in the filled missing data without resorting to filling with zeroes, averages, or duplicates.

\section{}

\subsection{Contributions}
Leo: Part A Q1, Part 2 Q1 \& 2\\
Longyue: Part A Q3, Part B Q3 \& 4\\
Gursewak: Part A Q2 \&4, Part B Q3\\
All group members completed course evaluations

\end{document}